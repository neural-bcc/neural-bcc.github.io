<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="NeuralFieldsBeyondCams">
  <meta name="keywords" content="NeuralFieldsBeyondCams">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta property="og:image" content="https://neural-bcc.github.io/static/img/thumbnail.jpg" />
  <title>Neural Fields Beyond Conventional Cameras</title>
  
  <meta property="og:description" content="Neural Fields Beyond Conventional Cameras"/>
  <link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">
  <script defer src="assets/fontawesome.all.min.js"></script>

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:creator" content="@mmalex">
  <meta name="twitter:title" content="Neural Fields Beyond Conventional Cameras">
  <meta name="twitter:description" content="The second workshop on neural fields beyond conventional cameras, hosted at CVPR 2025.">
  <meta name="twitter:image"  content="https://neural-bcc.github.io/static/img/thumbnail.jpg">

  <link rel="icon" href="./static/img/camera.png">

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">
  <link rel="stylesheet" href="css/style.css"> <!-- Resource style -->
  <script src="js/modernizr.js"></script> <!-- Modernizr -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>

  <style>
    .rcorners1 {
      border-radius: 10px;
      background: #ffffffd0;
      padding: 5px;
      font-size: 120%;
      color: #5c5c5c;
    }

    .button {
      border-radius: 10px;
      background: #ffffffd0;
      padding: 5px 15px 5px 15px;
    }

    .dropdown {
      position: relative;
      display: inline-block;
    }

    .dropdown-content {
      display: none;
      position: absolute;
      /* background-color: #f9f9f9; */
      min-width: 140px;
      box-shadow: 0px 8px 16px 0px rgba(0, 0, 0, 0.2);
      padding: 5px 15px 5px 15px;
      z-index: 1;
    }

    .dropdown:hover .dropdown-content {
      display: block;
    }
  </style>
</head>

<section class="hero" style="position: relative; height: 45vh; background-image: url('./static/img/background.jpg'); background-size: cover; background-position: 70% center; margin-bottom: 2vh;">
  <div style="position: absolute; bottom: 0; left: 0; right: 0; height: 2.5%; background: linear-gradient(to bottom, rgba(255, 255, 255, 0) 0%, rgba(255, 255, 255, 1) 100%); pointer-events: none;"></div>
  <div class="hero-body" style="display: flex; align-items: flex-end;">
    <div class="container has-text-justified">

      <h1 class="subtitle" style="font-size: 2rem; color: #fff; 
      text-shadow: 
          0 0 8px rgba(0, 0, 0, 1), 
          0 0 8px rgba(0, 0, 0, 1),
          0 0 8px rgba(0, 0, 0, 1);">
      CVPR 2025 - 2<sup>nd</sup> Workshop on
  </h1>
  
  <h1 class="title" style="font-size: 3rem; color: #fff; 
      text-shadow: 
          0 0 8px rgba(0, 0, 0, 1), 
          0 0 8px rgba(0, 0, 0, 1),
          0 0 8px rgba(0, 0, 0, 1);">
      Neural Fields Beyond Conventional Cameras
  </h1>
  
    </div>
  </div>
</section>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
              <!-- <p class="title is-1 publication-title">2<sup>nd</sup> Workshop on Neural Fields Beyond Conventional Cameras
              </p> -->
            </h1>
            <h1 style="font-size: 1.5rem">
            <b>Time:</b> June 11th 1-6PM CDT, CVPR 2025<br>
            <b>Location:</b> Room 106A, Music City Center, Nashville, Tennessee<br>
            <b>Posters:</b> ExHall D, Poster Boards #217 - #241, 3-4PM CDT<br> <br>
            <a href="https://princeton.zoom.us/j/92542081195?pwd=R3vr1itNRZQaGV7jGORQZcvUGH7dVW.1" target="_blank">Zoom Meeting Link</a><br>
            <br></h1>
          </div>
        </div>
      </div>
    </div>
    </div>
  </section>


  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h2 class="subtitle has-text-centered">
          <a href="#schedule" class="button">Schedule</a>
          <a href="#speakers" class="button">Speakers</a>
          <!-- <a href="#call4paper" class="button">Call for Papers</a> -->
          <a href="#accepted" class="button">Accepted Papers</a>
          <a href="#relatedwork" class="button">Related Work</a>
          <a href="#organizers" class="button">Organizers</a>
          <a href="#previous" class="button">Previous Editions</a>
        </h2>
        
      </div>
    </div>
  </section>


    <div class="container is-max-desktop">
      <section class="section" id="Intro">
        <div class="container is-max-desktop content">
          <div class="content has-text-justified">
            <p>Welcome to the official site of the 2nd Workshop on Neural Fields Beyond Conventional Cameras! This workshop will be held in conjunction with <b>CVPR 2025</b> June 11th-15th 2025.
            </div></p>
        </div>
      </section>

  <!-- ############################## -->
  <!-- motivation -->
  <section class="section" style="margin-top: -50px">
    <div class="container is-max-desktop">
      <section class="section" id="Motivation">
        <div class="container is-max-desktop content">
          <h2 class="title">Motivation</h2
          <div class="content has-text-justified">
            Neural fields have been widely adopted for learning novel view synthesis and 3D reconstruction from RGB images by modelling transport of light in the visible spectrum. This workshop focuses on <em>neural fields beyond conventional cameras</em>, including (1) learning neural fields from data from different sensors across the electromagnetic spectrum and beyond, such as lidar, cryo-electron microscopy (cryoEM), thermal, event cameras, acoustic, and more, and (2) modelling associated physics-based differentiable forward models and/or the physics of more complex light transport (reflections, shadows, polarization, diffraction limits, optics, scattering in fog or water, etc.). Our goal is to bring together a diverse group of researchers using neural fields across sensor domains to foster learning and discussion in this growing area. 
          </div>
        </div>
      </section>

    <section class="section" style="margin-top: -50px">
      <div class="container is-max-desktop">

        <!-- scheduler -->
      <section class="section" id="schedule">
        <div class="container is-max-desktop content">
          <h2 class="title">Schedule</h2>
          <div class="content has-text-justified">
            <table class="table table-striped">
              <tr>
                <td width="130">1:00 - 1:05pm</td>
                <td width="500" style="background-color:#e4ffc2">Welcome & Introduction</td>
                <td></td>
              </tr>
              <tr>
                <td>1:05 - 1:35pm</td>
                <td style="background-color:#cae1ff">Keynote: <b>Splatting Beyond the Visible Spectrum: Gaussian Splatting for Radar, Sonar, and More</b><br>Speaker: <em>Katherine Skinner</em></td>
                <td></td>
              </tr>
              <tr>
                <td>1:35 - 2:05pm</td>
                <td style="background-color:#cae1ff">Keynote: <b>Time-of-Flight Neural Fields</b><br>Speaker: <em>Christian Richardt</em></td>
                <td></td>
              </tr>
              <tr>
                <td>2:05 - 2:35pm</td>
                <td style="background-color:#cae1ff">Keynote: <b>Neural Fields for All: Physics, World Models, and Beyond</b><br>Speaker: <em>Jingyi Yu</em></td>
                <td></td>
              </tr>
              <tr>
                <td>2:35 - 2:45pm</td>
                <td style="background-color:#ffe0c6; text-align: left">Paper Spotlight 1: <b>Self-Calibrating Gaussian Splatting for Large Field of View Reconstruction</b><br>Authors: <em>Youming Deng, Wenqi Xian, Guandao Yang, Leonidas Guibas, Gordon Wetzstein, Steve Marschner, Paul Debevec</em></td>
                <td></td>
              </tr>
              <tr>
                <td>2:45 - 2:55pm</td>
                <td style="background-color:#ffe0c6; text-align: left">Paper Spotlight 2: <b>Neural Refraction Fields for Image Verification</b> <br> Authors: <em>Sage Simhon, Jingwei Ma, Prafull Sharma, Lucy Chai, Yen-Chen Lin, Phillip Isola</em></td>
                <td></td>
              </tr>
              <tr>
                <td>2:55 - 3:05pm</td>
                <td style="background-color:#ffe0c6; text-align: left">Paper Spotlight 3: <b>Hyperspectral Neural Radiance Fields</b> <br>Authors: <em>Gerry Chen, Sunil Kumar Narayanan, Thomas Gautier Ottou, Benjamin Missaoui, Harsh Muriki, Yongsheng Chen, Cédric Pradalier</em></td>
                <td></td>
              </tr>
              <tr>
                <td>3:05 - 4:00pm</td>
                <td style="background-color:#e4ffc2">Poster Session & Coffee Break</td>
                <td></td>
              </tr>
              <tr>
                <td>4:00 - 4:30pm</td>
                <td style="background-color:#cae1ff">Keynote: <b>Multi-modal Neural Fields for Robot Perception and Planning</b><br>Speaker: <em>Felix Heide</em></td>
                <td></td>
              </tr>
              <tr>
                <td>4:30 - 5:00pm</td>
                <td style="background-color:#cae1ff">Keynote: <b>Reconstructing the Cosmos with Physics Constrained Neural Fields</b><br>Speaker: <em>Aviad Levis</em></td>
                <td></td>
              </tr>
              <tr>
                <td>5:00 - 5:30pm</td>
                <td style="background-color:#cae1ff">Keynote: <b>Volume Representations for Inverse Problems</b><br>Speaker: <em>Sara Fridovich-Keil</em></td>
                <td></td>
              </tr>                
              <tr>
                <td>5:30 - 6:00pm</td>
                <td style="background-color:#e4ffc2">Panel Discussion<br>Moderator: David Lindell</td>
                <td></td>
              </tr>
            </table>
          </div>
        </div>
      </section>


        <!-- ############################## -->
        <!-- invited speakers -->
        <section class="section" id="invited-speakers">
          <div class="container is-max-desktop content">
            <h2 class="title" id="speakers">Keynote Speakers</h2>
        
<!-- 6. Keynote: Katherine Skinner -->
<a href="https://sites.google.com/umich.edu/kskin">
  <div class="card">
    <div class="card-content">
      <div class="columns is-vcentered">
        <div class="column is-one-quarter">
          <figure class="image is-128x128">
            <img class="is-rounded" src="static/img/katherine.png" alt="Katherine Skinner">
          </figure>
        </div>
        <div class="column">
          <p class="title is-4">Katherine Skinner</p>
          <p class="subtitle is-6">University of Michigan</p>
        </div>
      </div>
      <div class="content">
        Katherine Skinner is an Assistant Professor in the Department of Robotics at the University of Michigan, with a courtesy appointment in the Department of Naval Architecture and Marine Engineering. Before joining Michigan, she was a Postdoctoral Fellow at Georgia Institute of Technology in the Daniel Guggenheim School of Aerospace Engineering and the School of Earth and Atmospheric Sciences. She earned her M.S. and Ph.D. from the Robotics Institute at the University of Michigan, where she worked in the Deep Robot Optical Perception Laboratory. Her research focuses on robotics, computer vision, and machine learning to enable autonomy in dynamic, unstructured, or remote environments. Her dissertation advanced machine learning methods for underwater robotic perception, and she has collaborated with the Ford Center for Autonomous Vehicles to enhance urban perception.
      </div>
    </div>
  </div>
</a>

<!-- 5. Keynote: Christian Richardt -->
<a href="https://richardt.name/" target="_blank">
  <div class="card">
    <div class="card-content">
      <div class="columns is-vcentered">
        <div class="column is-one-quarter">
          <figure class="image is-128x128">
            <img class="is-rounded" src="static/img/christian.png" alt="Christian Richardt">
          </figure>
        </div>
        <div class="column">
          <p class="title is-4">Christian Richardt</p>
          <p class="subtitle is-6">Meta Reality Labs</p>
        </div>
      </div>
      <div class="content">
        Christian Richardt is a Research Scientist at Meta Reality Labs in Zurich, Switzerland, and previously at the Codec Avatars Lab in Pittsburgh, USA. He was previously a Reader (=Associate Professor) and EPSRC-UKRI Innovation Fellow in the Visual Computing Group and the CAMERA Centre at the University of Bath. His research interests cover the fields of image processing, computer graphics and computer vision, and his research combines insights from vision, graphics and perception to reconstruct visual information from images and videos, to create high-quality visual experiences with a focus on novel-view synthesis.
      </div>
    </div>
  </div>
</a>

  <!-- 1. Keynote: Jingyi Yu -->
<a href="http://www.yu-jingyi.com/cv/" target="_blank">
  <div class="card">
    <div class="card-content">
      <div class="columns is-vcentered">
        <div class="column is-one-quarter">
          <figure class="image is-128x128">
            <img class="is-rounded" src="static/img/jingyi.png" alt="Jingyi Yu">
          </figure>
        </div>
        <div class="column">
          <p class="title is-4">Jingyi Yu</p>
          <p class="subtitle is-6">ShanghaiTech University</p>
        </div>
      </div>
      <div class="content">
        Jingyi Yu is a professor and executive dean of the School of Information Science and Technology at ShanghaiTech University. He received his B.S. from Caltech in 2000 and his Ph.D. from MIT in 2005, and he is also affiliated with the University of Delaware. His research focuses on computer vision and computer graphics, particularly in computational photography and non-conventional optics and camera designs. His research has been generously supported by the National Science Foundation (NSF), the National Institute of Health, the Army Research Office, and the Air Force Office of Scientific Research (AFOSR). He is a recipient of the NSF CAREER Award, the AFOSR YIP Award, and the Outstanding Junior Faculty Award at the University of Delaware.
      </div>
    </div>
  </div>
</a>


<!-- 4. Keynote: Felix Heide -->
<a href="https://www.cs.princeton.edu/~fheide/" target="_blank">
  <div class="card">
    <div class="card-content">
      <div class="columns is-vcentered">
        <div class="column is-one-quarter">
          <figure class="image is-128x128">
            <img class="is-rounded" src="static/img/felix.png" alt="Felix Heide">
          </figure>
        </div>
        <div class="column">
          <p class="title is-4">Felix Heide</p>
          <p class="subtitle is-6">Princeton University</p>
        </div>
      </div>
      <div class="content">
        Felix Heide is a professor of Computer Science at Princeton University and heads the Princeton Computational Imaging Lab. He received his Ph.D. from the University of British Columbia and completed his postdoctoral fellowship at Stanford University. He is recognized as a SIGGRAPH Significant New Researcher, Sloan Research Fellow, and Packard Fellow. Previously, he founded an autonomous driving startup Algolux, later acquired by Torc and Daimler Trucks. His research focuses on imaging and computer vision techniques that help devices capture details in challenging conditions, spanning optics, machine learning, optimization, computer graphics, and computer vision.
      </div>
    </div>
  </div>
</a>

<!-- 3. Keynote: Aviad Levis -->
<a href="https://www.aviadlevis.info/" target="_blank">
  <div class="card">
    <div class="card-content">
      <div class="columns is-vcentered">
        <div class="column is-one-quarter">
          <figure class="image is-128x128">
            <img class="is-rounded" src="static/img/aviad.png" alt="Aviad Levis">
          </figure>
        </div>
        <div class="column">
          <p class="title is-4">Aviad Levis</p>
          <p class="subtitle is-6">University of Toronto</p>
        </div>
      </div>
      <div class="content">
        Aviad Levis is an assistant professor in the Departments of Computer Science and Astronomy and Astrophysics at the University of Toronto. He is an associated faculty member at the Dunlap Institute for Astronomy and Astrophysics. His research focuses on scientific computational imaging and AI for science. Prior to that, he was a postdoctoral scholar in the Department of Computing and Mathematics at Caltech, supported by the Zuckerman and Viterbi postdoctoral fellowships, working with Katie Bouman on imaging the galactic center black hole as part of the Event Horizon Telescope collaboration. He received his Ph.D. (2020) from the Technion and B.Sc. (2013) from Ben-Gurion University. His Ph.D. thesis into tomography of clouds has paved the way for an ERC-funded space mission (CloudCT) led by his Ph.D. advisor Yoav Schechner.
      </div>
    </div>
  </div>
</a>



<!-- 2. Keynote: Sara Fridovich-Keil -->
<a href="https://sarafridov.github.io/" target="_blank">
  <div class="card">
    <div class="card-content">
      <div class="columns is-vcentered">
        <div class="column is-one-quarter">
          <figure class="image is-128x128">
            <img class="is-rounded" src="static/img/sara.png" alt="Sara Fridovich-Keil">
          </figure>
        </div>
        <div class="column">
          <p class="title is-4">Sara Fridovich-Keil</p>
          <p class="subtitle is-6">Georgia Tech</p>
        </div>
      </div>
      <div class="content">
        Sara Fridovich-Keil is an assistant professor at Georgia Tech's Department of Electrical and Computer Engineering. She completed her postdoctoral research at Stanford University under the guidance of Gordon Wetzstein and Mert Pilanci, after earning her Ph.D. in Electrical Engineering and Computer Sciences from UC Berkeley, where she was advised by Ben Recht. Her work focuses on machine learning, signal processing, and optimization to address inverse problems in computer vision as well as in computational, medical, and scientific imaging. Her research aims to identify optimal signal representations while balancing interpretability and computational efficiency.
      </div>
    </div>
  </div>
</a>

            </div>
          </div>
        </a>

        <!-- ############################## -->
        <!-- accepted papers -->
        <section class="section" id="accepted">
          <div class="container is-max-desktop content">
            <h2 class="title">Accepted Papers</h2>
            <div class="content has-text-justified">
              <ul>
                <li><strong>(Spotlight ⭐) &nbsp; </strong> <a href="https://denghilbert.github.io/self-cali/">Self-Calibrating Gaussian Splatting for Large Field of View Reconstruction</a><br><em>Youming Deng, Wenqi Xian, Guandao Yang, Leonidas Guibas, Gordon Wetzstein, Steve Marschner, Paul Debevec</em></li><br>
                <li><strong>(Spotlight ⭐) &nbsp; </strong> <a href="https://drive.google.com/file/d/1KN9xwIgrsLe1e2qgKLW5yedq3dROlAF-/view?usp=drive_link">Neural Refraction Fields for Image Verification</a><br><em>Sage Simhon, Jingwei Ma, Prafull Sharma, Lucy Chai, Yen-Chen Lin, Phillip Isola</em></li><br>
                <li><strong>(Spotlight ⭐) &nbsp; </strong><a href="https://gchenfc.github.io/hs-nerf-website/">Hyperspectral Neural Radiance Fields</a><br><em>Gerry Chen, Sunil Kumar Narayanan, Thomas Gautier Ottou, Benjamin Missaoui, Harsh Muriki, Yongsheng Chen, Cédric Pradalier</em></li><br>
                <li><a href="https://openaccess.thecvf.com/content/WACV2025/papers/Ichimaru_Neural_SDF_for_Shadow-Aware_Unsupervised_Structured_Light_WACV_2025_paper.pdf">Neural SDF for Shadow-aware Unsupervised Structured Light</a><br><em>Kazuto Ichimaru, Diego Thomas, Takafumi Iwaguchi, Hiroshi Kawasaki</em></li><br>
                <li><a href="https://hg-chung.github.io/Interpretable-Inverse-Rendering/">Differentiable Inverse Rendering with Interpretable Basis BRDFs</a><br><em>Hoon-Gyu Chung, Seokjun Choi, Seung-Hwan Baek</em></li><br>
                <li><a href="https://cuiziteng.github.io/Luminance_GS_web/">Luminance-GS: Adapting 3D Gaussian Splatting to Challenging Lighting Conditions with View-Adaptive Curve Adjustment</a><br><em>Ziteng Cui, Xuangeng Chu, Tatsuya Harada</em></li><br>
                <li><a href="https://visual.cs.brown.edu/gftorf">Time of the Flight of the Gaussians: Optimizing Depth Indirectly in Dynamic Radiance Fields</a><br><em>Runfeng Li, Mikhail Okunev, Zixuan Guo, Anh Ha Duong, Christian Richardt, Matthew O'Toole, James Tompkin</em></li><br>
                <li><a href="https://bchao1.github.io/gaussian-wave-splatting/">Gaussian Wave Splatting for Computer Generated Holography</a><br><em>Suyeon Choi*, Brian Chao*, Jackie Yang, Manu Gopakumar, Gordon Wetzstein</em></li><br>
                <li><a href="https://mezzelfo.github.io/EOGS/">Gaussian Splatting for Efficient Satellite Image Photogrammetry</a><br><em>Luca Savant Aira, Gabriele Facciolo, Thibaud Ehret</em></li><br>
                <li><a href="https://ieeexplore.ieee.org/document/10685550">Z-Splat: Z-Axis Gaussian Splatting for Camera-Sonar Fusion</a><br><em>Ziyuan Qu, Omkar Vengurlekar, Mohamad Qadri, Kevin Zhang, Michael Kaess, Christopher Metzler, Suren Jayasuriya, and Adithya Pediredla</em></li><br>
                <li><a href="https://yujiewang.info/projects/dof_gs/">DOF-GS: Adjustable Depth-of-Field 3D Gaussian Splatting for Post-Capture Refocusing, Defocus Rendering and Blur Removal</a><br><em>Yujie Wang, Praneeth Chakravarthula, Baoquan Chen</em></li><br>
                <li><a href="https://lttm.github.io/MultimodalStudio/">MultimodalStudio: A Heterogeneous Sensor Dataset and Framework for Neural Rendering across Multiple Imaging Modalities</a><br><em>Federico Lincetto, Gianluca Agresti, Mattia Rossi, Pietro Zanuttigh</em></li><br>
                <li><a href="https://cvpr.thecvf.com/virtual/2025/poster/34929">HyperGS: Hyperspectral 3D Gaussian Splatting</a><br><em>Christopher Thirgood, Oscar Mendez, Erin Ling, Simon Hadfield</em></li><br>
                <li><a href="https://arxiv.org/abs/2506.01389?context=cs.CV">Neural shape reconstruction from multiple views with static pattern projection</a><br><em>Ryo Furukawa, Kota Nishihara, Hiroshi Kawasaki</em></li><br>
                <li><a href="https://s3anwu.github.io/pbrnerf">PBR-NeRF: Inverse Rendering with Physics-Based Neural Fields</a><br><em>Sean Wu, Shamik Basu, Tim Broedermann, Luc Van Gool, Christos Sakaridis</em></li><br>
                <li><a href="https://umautobots.github.io/lihi_gs">LiHi-GS: LiDAR-Supervised Gaussian Splatting for Highway Driving Scene Reconstruction</a><br><em>Pou-Chun Kung, Xianling Zhang, Katherine A Skinner, Nikita Jaipuria</em></li><br>
                <li><a href="https://flash-splat.github.io/">Flash-Splat: 3D Reflection Removal with Flash Cues and Gaussian Splats</a><br><em>Mingyang Xie, Haoming Cai, Sachin Shah, Yiran Xu, Brandon Y Feng, Jia-Bin Huang, Christopher A Metzler</em></li><br>
                <li><a href="https://github.com/poloclub/3d-gaussian-splat-attack">3D Gaussian Splatting Vulnerabilities</a><br><em>Matthew Hull, Haoyang Yang, Pratham Mehta, Mansi Phute, Aeree Cho, Haoran Wang, Matthew Lau, Wenke Lee, Willian Lunardi, Martin Andreoni, Duen Horng Chau</em></li><br>
                <li><a href="https://jho-yonsei.github.io/SMURF/">SMURF: Continuous Dynamics for Motion-Deblurring Radiance Fields</a><br><em>Jungho Lee, Dogyoon Lee, Minhyeok Lee, Donghyeong Kim, Sangyoun Lee</em></li><br>
                <li><a href="https://jho-yonsei.github.io/CoCoGaussian/">CoCoGaussian: Leveraging Circle of Confusion for Gaussian Splatting from Defocused Images</a><br><em>Jungho Lee, Suhwan Cho, Taeoh Kim, Ho-Deok Jang, Minhyeok Lee, Geonho Cha, Dongyoon Wee, Dogyoon Lee, Sangyoun Lee</em></li><br>
                <li><a href="https://kushalvyas.github.io/alpine-docs/">Alpine - A Flexible, User-friendly, Distributed Library for Implicit Neural Representations</a><br><em>Kushal Vyas, Vishwanath Saragadam, Ashok Veeraraghavan, Guha Balakrishnan</em></li><br>
                <li><a href="https://arxiv.org/abs/2406.02972">Event3DGS: Event-Based 3D Gaussian Splatting for High-Speed Robot Egomotion</a><br><em>Tianyi Xiong, Jiayi Wu, Botao He, Cornelia Fermuller, Yiannis Aloimonos, Heng Huang, Christopher A. Metzler</em></li><br>
                <li><a href="https://convexsplatting.github.io/">3D Convex Splatting: Radiance Field Rendering with 3D Smooth Convexes</a><br><em>Jan Held*, Renaud Vandeghen*, Abdullah Hamdi*, Adrien Deliege, Anthony Cioppa, Silvio Giancola, Andrea Vedaldi, Bernard Ghanem, Marc Van Droogenbroeck</em></li><br>
                <li><a href="https://arxiv.org/abs/2504.15262">Revealing the 3D Cosmic Web through Gravitationally Constrained Neural Fields</a><br><em>Brandon Zhao, Aviad Levis, Liam Connor, Pratul P. Srinivasan, Katherine L. Bouman</em></li><br>
                <li><a href="https://drive.google.com/file/d/1qn6BxhM8XOggMyOEsM_zZknQmKjlTinm/view?usp=drive_link">DBMovi-GS: Dynamic View Synthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting</a><br><em>Yeon-Ji Song, Jaein Kim, Byung Ju Kim, Byoung-Tak Zhang</em></li><br>
                <li><a href="https://drive.google.com/file/d/1Qt0EgbDEEPwOe0FkUrS-trYPWGARcXdW/view?usp=drive_link">Reconstruction Using the Invisible: Intuition from NIR and Metadata for Enhanced 3D Gaussian Splatting</a><br><em>Gyusam Chang, Tuan-Anh Vu, Vivek Alumootil, Harris Song, Deanna Pham, Sangpil Kim, M. Khalid Jawed</em></li><br>
                <li><a href="https://drive.google.com/file/d/1SevccaCtsFjrPEACttfLwfhOhhWBbWnp/view?usp=drive_link">Joint Attitude Estimation and 3D Neural Reconstruction of non-cooperative space objects</a><br><em>Clément Forray, Pauline Delporte, Nicolas Delaygue, Florence Genin, Dawa Derksen</em></li><br>
                <li><a href="https://drive.google.com/file/d/1OEIbFpF-tAcR-NNZOal-rWC6aYs5SAnR/view?usp=drive_link">HessianForge: Scalable LiDAR reconstruction with Physics-Informed Neural Representation and Smoothness Energy Constraints</a><br><em>Hrishikesh Viswanath, Md Ashiqur Rahman, Chi Lin, Damon Conover, Aniket Bera</em></li><br>
                <li><a href="https://drive.google.com/file/d/1gJqQMNMdxuPBr8iO1oPsKjHKHr8n_tX0/view?usp=drive_link">RadarSplat: Radar Gaussian Splatting for High-Fidelity Data Synthesis and 3D Reconstruction of Autonomous Driving Scenes</a><br><em>Pou-Chun Kung, Skanda Harisha, Ram Vasudevan, Aline Eid, Katherine A. Skinner</em></li><br>
                <li><a href="https://drive.google.com/file/d/1mi0W5y5oEEfBsbMB9GUWqiOtpNhnKcuv/view?usp=drive_link">SHaDe: Compact and Consistent Dynamic 3D Reconstruction via Tri-Plane Deformation and Latent Diffusion</a><br><em>Asrar Alruwayqi</em></li><br>
                <li><a href="https://drive.google.com/file/d/1Bvw5snUMBI1Y-P_uqGjwIeokLOTWIw_d/view?usp=drive_link">EchoNeRF: Generalizable Neural Radiance Fields for Novel Echocardiographic View Synthesis</a><br><em>Yuehao Wang, Edward Mei, Zhangyang Wang, Gregory Holste</em></li><br>
                <li><a href="https://drive.google.com/file/d/1hjN90hH1vaXINQ8U5NgRwn2PIY7cbuQ-/view?usp=drive_link">ToFGS: Temporally-Resolved Inverse Rendering with Gaussian Splatting for Time-of-Flight</a><br><em>Omkar Shailendra Vengurlekar, Aaron Saju Augustine, Suren Jayasuriya</em></li><br>
              </ul>
            </div>
          </div>
        </section>



        <!-- ############################## -->
        <!-- call for papers -->
        <!--
        <section class="section" id="Call for papers">
          <div class="container is-max-desktop content">
            <h2 class="title" id="call4paper">Call for Papers</h2>
            This workshop aims to bring together a diverse group of researchers using neural fields, gaussian splatting (GS), or other stochastically-optimized methods across a wide range of sensor domains. We recommend looking through the <a href="#relatedwork">related works</a> to explore the breadth of work in this area. We solicit <em><b>non-archival</b></em> papers (which will <em><b>not</b></em> be published in proceedings) on topics including but not limited to:
            <ul>
              <li>Neural field/GS-based reconstruction and view synthesis using non-RGB sensor measurements (LiDAR, Thermal, Event, CT, MRI, Ultrasound, Cryo-EM, Sonar, etc)</li>
              <li>Neural fields/GS for computational imaging</li>
              <li>Neural fields/GS for sensor modelling and calibration</li>
              <li>Neural fields/GS for modelling visual cues (shadows, reflections, material, etc)</li>
              <li>Applications of the above to autonomous vehicles, AR/VR/XR, robotics, medicine, scientific discovery, and beyond</li>
            </ul>
            Of the submissions, <b>three</b> will be selected by the review committee as <b>spotlight works</b>. Spotlight papers will each receive a 10-minute presentation slot in the main schedule, and all accepted papers will be able to present a poster in the workshop. We encourage submissions from both new and experienced researchers — this is a great opportunity to present your research to a broader audience, receive feedback on your work, and connect with other researchers in the field.


            <h2 class="title" id="">Style and Author Instructions</h2>
          </div>
        </section>
        -->



        <section class="section" id="Related works">
          <div class="container is-max-desktop content">
            <h2 class="title" id="relatedwork">Related Works</h2>
            Below is a collection of example works on neural fields beyond conventional cameras:
            <ul>
              <li>
                <a href="https://arxiv.org/abs/2303.16254">
                  CryoFormer: Continuous Heterogeneous Cryo-EM Reconstruction using Transformer-based Neural Representations
                </a>
                ICLR 2024
              </li>
              
              
              <li>
                <a href="https://arxiv.org/abs/2405.04662">
                  Radar Fields: Frequency-Space Neural Scene Representations for FMCW Radar
                </a>
                SIGGRAPH 2024
              </li>
              
              <li><a href="https://arxiv.org/abs/2312.14239">	
                PlatoNeRF: 3D Reconstruction in Plato's Cave via Single-View Two-Bounce Lidar</a> CVPR 2024
              </li>
              <li><a href="https://arxiv.org/abs/2312.05247">Dynamic LiDAR Re-simulation using Compositional Neural Fields</a> CVPR 2024
              </li>
              <li><a href="https://arxiv.org/abs/2305.16321">Eclipse: Disambiguating Illumination and Materials using Unintended Shadows</a> CVPR 2024 </li>
              <li>
                <a href="https://arxiv.org/pdf/2311.17396">
                  Spectral and Polarization Vision: Spectro-polarimetric Real-world Dataset
                </a>
                CVPR 2024
              </li>

              <li>
                <a href="https://arxiv.org/abs/2306.12562">
                  Neural Spectro-polarimetric Fields
                </a>
                SIGGRAPH ASIA 2023
              </li>
              <li><a href="https://arxiv.org/abs/2208.11300">E-NeRF: Neural Radiance Fields from a Moving Event Camera</a> RA-L 2023 </li>
              <li><a href="https://arxiv.org/abs/2301.10520">Ultra-NeRF: Neural Radiance Fields for Ultrasound Imaging</a> MIDL 2023 </li>
              <li><a href="https://arxiv.org/abs/2307.09555">Transient Neural Radiance Fields for Lidar View Synthesis and 3D Reconstruction</a> NeurIPS 2023 </li>
              <li><a href="https://arxiv.org/abs/2305.01643">	
                Neural LiDAR Fields for Novel View Synthesis</a> ICCV 2023
              </li>
              <li><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Shandilya_Neural_Fields_for_Structured_Lighting_ICCV_2023_paper.pdf">Neural Fields for Structured Lighting</a> ICCV 2023 </li>
              <li><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Qi_E2NeRF_Event_Enhanced_Neural_Radiance_Fields_from_Blurry_Images_ICCV_2023_paper.pdf">	
                E<sup>2</sup>NeRF: Event Enhanced Neural Radiance Fields from Blurry Images</a> ICCV 2023
              </li>
              <li><a href="https://arxiv.org/abs/2212.04531">ORCa: Glossy Objects as Radiance Field Cameras</a> CVPR 2023 </li>
              <li><a href="https://arxiv.org/abs/2305.01652">Humans as Light Bulbs: 3D Human Reconstruction from Thermal Reflection</a> CVPR 2023 </li>
              <li><a href="https://arxiv.org/abs/2304.07743">SeaThru-NeRF: Neural Radiance Fields in Scattering Media</a> CVPR 2023 </li>
              <li><a href="https://arxiv.org/abs/2206.11896">EventNeRF: Neural Radiance Fields from a Single Colour Event Camera</a> CVPR 2023 </li>
              <li><a href="https://ojs.aaai.org/index.php/AAAI/article/view/20171">Neural Interferometry: Image Reconstruction from Astronomical Interferometers Using Transformer-Conditioned Neural Fields</a> AAAI 2022 </li>
              <li><a href="https://arxiv.org/abs/2204.00628">Learning Neural Acoustic Fields</a> NeurIPS 2022 </li>
              <li><a href="https://arxiv.org/abs/2203.13458">	
                PANDORA: Polarization-Aided Neural Decomposition Of Radiance</a> ECCV 2022
              </li>                
              <li><a href="https://arxiv.org/abs/2202.01020">Medical Neural Radiance Fields for Reconstructing 3D-aware CT-Projections from a Single X-ray</a> EMBC 2022
              </li>
              <br>
            </ul>
            and many more ...


            </ul>
          </div>
        </section>
          
          


        <section class="section" id="Organizers">
          <div class="container is-max-desktop content">
            <h2 class="title" id="organizers">Organizers</h2>

        <div class="columns is-multiline">
          <!-- Ilya Chugunov -->
          <div class="column is-one-quarter">
            <a href="https://ilyac.info/">
              <div class="card">
                <div class="card-image">
                  <figure class="image">
                    <img class="is-rounded" src="./static/img/ilya.png" alt="Image of Ilya Chugunov">
                  </figure>
                </div>
                <div class="card-content">
                  <div class="media">
                    <div class="media-content" style="overflow-x: unset;">
                      <p class="title is-6" style="margin-bottom: 0vh;">Ilya Chugunov</p>
                      <p class="subtitle is-6 has-text-left has-text-grey" style="height: 0.5rem;"><br> <em>(primary)</em></p>
                      <p class="subtitle is-6" style="height: 2.2rem;"><br> Princeton</p>
                    </div>
                  </div>
                </div>
              </div>
            </a>
          </div>
        
          <!-- Tzofi Klinghoffer -->
          <div class="column is-one-quarter">
            <a href="https://tzofi.github.io/">
              <div class="card">
                <div class="card-image">
                  <figure class="image">
                    <img class="is-rounded" src="./static/img/tzofi.png" alt="Image of Tzofi Klinghoffer">
                  </figure>
                </div>
                <div class="card-content">
                  <div class="media">
                    <div class="media-content" style="overflow-x: unset;">
                      <p class="title is-6" style="margin-bottom: 0vh;">Tzofi Klinghoffer</p>
                      <p class="subtitle is-6 has-text-left has-text-grey" style="height: 0.5rem;"><br> <em>(secondary)</em></p>
                      <p class="subtitle is-6" style="height: 2.2rem;"><br>MIT Media Lab</p>
                    </div>
                  </div>
                </div>
              </div>
            </a>
          </div>

<!-- Shengyu Huang -->
<div class="column is-one-quarter">
  <a href="https://shengyuh.github.io/">
    <div class="card">
      <div class="card-image">
        <figure class="image">
          <img class="is-rounded" src="./static/img/shengyu.png" alt="Image of Shengyu Huang">
        </figure>
      </div>
      <div class="card-content">
        <div class="media">
          <div class="media-content" style="overflow-x: unset;">
            <p class="title is-6" style="margin-bottom: 2.5vh;">Shengyu Huang</p>
            <p class="subtitle is-6" style="height: 2rem;">ETH Zürich</p>
          </div>
        </div>
      </div>
    </div>
  </a>
</div>

<!-- Daniel Gilo -->
<div class="column is-one-quarter">
  <a href="https://scholar.google.com/citations?user=ARRwFY8AAAAJ&hl=en">
    <div class="card">
      <div class="card-image">
        <figure class="image">
          <img class="is-rounded" src="./static/img/daniel.png" alt="Image of Daniel Gilo">
        </figure>
      </div>
      <div class="card-content">
        <div class="media">
          <div class="media-content" style="overflow-x: unset;">
            <p class="title is-6" style="margin-bottom: 2.5vh;">Daniel Gilo</p>
            <p class="subtitle is-6" style="height: 2rem;">Technion</p>
          </div>
        </div>
      </div>
    </div>
  </a>
</div>

<!-- Wenzheng Chen -->
<div class="column is-one-quarter">
  <a href="https://wenzhengchen.github.io/">
    <div class="card">
      <div class="card-image">
        <figure class="image">
          <img class="is-rounded" src="./static/img/wenzheng.png" alt="Image of Wenzheng Chen">
        </figure>
      </div>
      <div class="card-content">
        <div class="media">
          <div class="media-content" style="overflow-x: unset;">
            <p class="title is-6" style="margin-bottom: 2.5vh;">Wenzheng Chen</p>
            <p class="subtitle is-6" style="height: 2rem;">Peking University</p>
          </div>
        </div>
      </div>
    </div>
  </a>
</div>

<!-- Akshat Dave -->
<div class="column is-one-quarter">
  <a href="https://akshatdave.github.io/">
    <div class="card">
      <div class="card-image">
        <figure class="image">
          <img class="is-rounded" src="./static/img/akshat.png" alt="Image of Akshat Dave">
        </figure>
      </div>
      <div class="card-content">
        <div class="media">
          <div class="media-content" style="overflow-x: unset;">
            <p class="title is-6" style="margin-bottom: 2.5vh;">Akshat Dave</p>
            <p class="subtitle is-6" style="height: 2rem;">MIT Media Lab</p>
          </div>
        </div>
      </div>
    </div>
  </a>
</div>

<!-- Lingjie Liu -->
<div class="column is-one-quarter">
  <a href="https://lingjie0206.github.io/">
    <div class="card">
      <div class="card-image">
        <figure class="image">
          <img class="is-rounded" src="./static/img/lingjie.png" alt="Image of Lingjie Liu">
        </figure>
      </div>
      <div class="card-content">
        <div class="media">
          <div class="media-content" style="overflow-x: unset;">
            <p class="title is-6" style="margin-bottom: 2.5vh;">Lingjie Liu</p>
            <p class="subtitle is-6" style="height: 2rem;">University of Pennsylvania</p>
          </div>
        </div>
      </div>
    </div>
  </a>
</div>

<!-- David Lindell -->
<div class="column is-one-quarter">
  <a href="https://davidlindell.com/">
    <div class="card">
      <div class="card-image">
        <figure class="image">
          <img class="is-rounded" src="./static/img/david.png" alt="Image of David Lindell">
        </figure>
      </div>
      <div class="card-content">
        <div class="media">
          <div class="media-content" style="overflow-x: unset;">
            <p class="title is-6" style="margin-bottom: 2.5vh;">David Lindell</p>
            <p class="subtitle is-6" style="height: 2rem;">University of Toronto</p>
          </div>
        </div>
      </div>
    </div>
  </a>
</div>

<!-- Or Litany -->
<div class="column is-one-quarter">
  <a href="https://orlitany.github.io/">
    <div class="card">
      <div class="card-image">
        <figure class="image">
          <img class="is-rounded" src="./static/img/or.png" alt="Image of Or Litany">
        </figure>
      </div>
      <div class="card-content">
        <div class="media">
          <div class="media-content" style="overflow-x: unset;">
            <p class="title is-6" style="margin-bottom: 2.5vh;">Or Litany</p>
            <p class="subtitle is-6" style="height: 2rem;">Technion, NVIDIA</p>
          </div>
        </div>
      </div>
    </div>
  </a>
</div>

<!-- Ramesh Raskar -->
<div class="column is-one-quarter">
  <a href="https://www.media.mit.edu/people/raskar/overview/">
    <div class="card">
      <div class="card-image">
        <figure class="image">
          <img class="is-rounded" src="./static/img/ramesh.png" alt="Image of Ramesh Raskar">
        </figure>
      </div>
      <div class="card-content">
        <div class="media">
          <div class="media-content" style="overflow-x: unset;">
            <p class="title is-6" style="margin-bottom: 2.5vh;">Ramesh Raskar</p>
            <p class="subtitle is-6" style="height: 2rem;">MIT Media Lab</p>
          </div>
        </div>
      </div>
    </div>
  </a>
</div>


        

        

        


        
        

        <section class="section" id="Previous Editions">
          <div class="container is-max-desktop content">
            <h2 class="title" id="previous">Previous Workshop Editions</h2>
            <li>
              <a href="2024/2024.html">2024 - 1st Edition @ European Conference on Computer Vision</a><br>
            </li>
          </div>
        </section>

          <footer class="footer">
            <div class="container">
              <div class="content">
                <p style="font-size: small;">
                  This website is licensed under a <a rel="license"
                    href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                    Commons Attribution-ShareAlike 4.0 International License</a>.
                  <br />
                  It borrows the source code of <a href="https://github.com/nerfies/nerfies.github.io">this website</a>, thanks to Utkarsh Sinha and Keunhong Park.
                </p>
              </div>
            </div>
          </footer>

        <!-- ############################## -->
        <!-- call for papers -->
        <!--
        <section class="section" id="Call for papers">
          <div class="container is-max-desktop content">
            <h2 class="title" id="call4paper">Call for Papers</h2>
            This workshop aims to bring together a diverse group of researchers using neural fields, gaussian splatting (GS), or other stochastically-optimized methods across a wide range of sensor domains. We recommend looking through the <a href="#relatedwork">related works</a> to explore the breadth of work in this area. We solicit <em><b>non-archival</b></em> papers (which will <em><b>not</b></em> be published in proceedings) on topics including but not limited to:
            <ul>
              <li>Neural field/GS-based reconstruction and view synthesis using non-RGB sensor measurements (LiDAR, Thermal, Event, CT, MRI, Ultrasound, Cryo-EM, Sonar, etc)</li>
              <li>Neural fields/GS for computational imaging</li>
              <li>Neural fields/GS for sensor modelling and calibration</li>
              <li>Neural fields/GS for modelling visual cues (shadows, reflections, material, etc)</li>
              <li>Applications of the above to autonomous vehicles, AR/VR/XR, robotics, medicine, scientific discovery, and beyond</li>
            </ul>
            Of the submissions, <b>three</b> will be selected by the review committee as <b>spotlight works</b>. Spotlight papers will each receive a 10-minute presentation slot in the main schedule, and all accepted papers will be able to present a poster in the workshop. We encourage submissions from both new and experienced researchers — this is a great opportunity to present your research to a broader audience, receive feedback on your work, and connect with other researchers in the field.


            <h2 class="title" id="">Style and Author Instructions</h2>
          </div>
        </section>
        -->
</body>
<script src="js/jquery-2.1.1.js"></script>
<script src="js/jquery.mobile.custom.min.js"></script> <!-- Resource jQuery -->
<script src="js/main.js"></script> <!-- Resource jQuery -->

</html>
