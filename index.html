<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="NeuralFieldsBeyondCams">
  <meta name="keywords" content="NeuralFieldsBeyondCams">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta property="og:image" content="https://neural-bcc.github.io/static/img/thumbnail.jpg" />
  <title>Neural Fields Beyond Conventional Cameras</title>
  
  <meta property="og:description" content="Neural Fields Beyond Conventional Cameras"/>
  <link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">
  <script defer src="assets/fontawesome.all.min.js"></script>

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:creator" content="@mmalex">
  <meta name="twitter:title" content="Neural Fields Beyond Conventional Cameras">
  <meta name="twitter:description" content="The second workshop on neural fields beyond conventional cameras, hosted at CVPR 2025.">
  <meta name="twitter:image"  content="https://neural-bcc.github.io/static/img/thumbnail.jpg">

  <link rel="icon" href="./static/img/camera.png">

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">
  <link rel="stylesheet" href="css/style.css"> <!-- Resource style -->
  <script src="js/modernizr.js"></script> <!-- Modernizr -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>

  <style>
    .rcorners1 {
      border-radius: 10px;
      background: #ffffffd0;
      padding: 5px;
      font-size: 120%;
      color: #5c5c5c;
    }

    .button {
      border-radius: 10px;
      background: #ffffffd0;
      padding: 5px 15px 5px 15px;
    }

    .dropdown {
      position: relative;
      display: inline-block;
    }

    .dropdown-content {
      display: none;
      position: absolute;
      /* background-color: #f9f9f9; */
      min-width: 140px;
      box-shadow: 0px 8px 16px 0px rgba(0, 0, 0, 0.2);
      padding: 5px 15px 5px 15px;
      z-index: 1;
    }

    .dropdown:hover .dropdown-content {
      display: block;
    }
  </style>
</head>

<section class="hero" style="position: relative; height: 45vh; background-image: url('./static/img/background.jpg'); background-size: cover; background-position: 70% center; margin-bottom: 2vh;">
  <div style="position: absolute; bottom: 0; left: 0; right: 0; height: 2.5%; background: linear-gradient(to bottom, rgba(255, 255, 255, 0) 0%, rgba(255, 255, 255, 1) 100%); pointer-events: none;"></div>
  <div class="hero-body" style="display: flex; align-items: flex-end;">
    <div class="container has-text-justified">

      <h1 class="subtitle" style="font-size: 2rem; color: #fff; 
      text-shadow: 
          0 0 8px rgba(0, 0, 0, 1), 
          0 0 8px rgba(0, 0, 0, 1),
          0 0 8px rgba(0, 0, 0, 1);">
      CVPR 2025 - 2<sup>nd</sup> Workshop on
  </h1>
  
  <h1 class="title" style="font-size: 3rem; color: #fff; 
      text-shadow: 
          0 0 8px rgba(0, 0, 0, 1), 
          0 0 8px rgba(0, 0, 0, 1),
          0 0 8px rgba(0, 0, 0, 1);">
      Neural Fields Beyond Conventional Cameras
  </h1>
  
    </div>
  </div>
</section>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
              <!-- <p class="title is-1 publication-title">2<sup>nd</sup> Workshop on Neural Fields Beyond Conventional Cameras
              </p> -->
            </h1>
            <h1 style="font-size: 1.5rem">
            <b>Date: June 11th PM, CVPR 2025</b><br>
            <b>Location: TBD, Music City Center, Nashville, Tennessee</b><br>
            <br></h1>
          </div>
        </div>
      </div>
    </div>
    </div>
  </section>


  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h2 class="subtitle has-text-centered">
          <a href="#schedule" class="button">Schedule</a>
          <a href="#speakers" class="button">Speakers</a>
          <a href="#call4paper" class="button">Call for Papers</a>
          <a href="#relatedwork" class="button">Related Work</a>
          <!-- <a href="#challenge" class="button">Challenge</a> -->
          <a href="#organizers" class="button">Organizers</a>
          <a href="#previous" class="button">Previous Editions</a>
        </h2>
        
      </div>
    </div>
  </section>


    <div class="container is-max-desktop">
      <section class="section" id="Intro">
        <div class="container is-max-desktop content">
          <div class="content has-text-justified">
            <p>Welcome to the official site of the 2nd Workshop on Neural Fields Beyond Conventional Cameras! This workshop will be held in conjunction with <b>CVPR 2025</b> June 11th-15th 2025. Additional information such as exact schedules and location will be posted here as we get closer to the event.
            </div></p>
        </div>
      </section>

  <!-- ############################## -->
  <!-- motivation -->
  <section class="section" style="margin-top: -50px">
    <div class="container is-max-desktop">
      <section class="section" id="Motivation">
        <div class="container is-max-desktop content">
          <h2 class="title">Motivation</h2
          <div class="content has-text-justified">
            Neural fields have been widely adopted for learning novel view synthesis and 3D reconstruction from RGB images by modelling transport of light in the visible spectrum. This workshop focuses on <em>neural fields beyond conventional cameras</em>, including (1) learning neural fields from data from different sensors across the electromagnetic spectrum and beyond, such as lidar, cryo-electron microscopy (cryoEM), thermal, event cameras, acoustic, and more, and (2) modelling associated physics-based differentiable forward models and/or the physics of more complex light transport (reflections, shadows, polarization, diffraction limits, optics, scattering in fog or water, etc.). Our goal is to bring together a diverse group of researchers using neural fields across sensor domains to foster learning and discussion in this growing area. 
          </div>
        </div>
      </section>

    <section class="section" style="margin-top: -50px">
      <div class="container is-max-desktop">

        <!-- scheduler -->
      <section class="section" id="schedule">
        <div class="container is-max-desktop content">
          <h2 class="title">Schedule (Tentative)</h2>
          <div class="content has-text-justified">
            <table class="table table-striped">
              <tr>
                <td width="130">1:00 - 1:05pm</td>
                <td width="500" style="background-color:#e4ffc2">Welcome & Introduction</td>
                <td></td>
              </tr>
              <tr>
                <td>1:05 - 1:35pm</td>
                <td style="background-color:#cae1ff">Keynote: Sara Fridovich-Keil</td>
                <td></td>
              </tr>
              <tr>
                <td>1:35 - 2:05pm</td>
                <td style="background-color:#cae1ff">Keynote: Aviad Levis</td>
                <td></td>
              </tr>
              <tr>
                <td>2:05 - 2:35pm</td>
                <td style="background-color:#cae1ff">Keynote: Jingyi Yu</td>
                <td></td>
              </tr>
              <tr>
                <td>2:35 - 2:45pm</td>
                <td style="background-color:#ffe0c6">Paper Spotlight 1 (TBD)</td>
                <td></td>
              </tr>
              <tr>
                <td>2:45 - 2:55pm</td>
                <td style="background-color:#ffe0c6">Paper Spotlight 2 (TBD)</td>
                <td></td>
              </tr>
              <tr>
                <td>2:55 - 3:05pm</td>
                <td style="background-color:#ffe0c6">Paper Spotlight 3 (TBD)</td>
                <td></td>
              </tr>
              <tr>
                <td>3:05 - 3:40pm</td>
                <td style="background-color:#e4ffc2">Poster Session & Coffee Break</td>
                <td></td>
              </tr>
              <tr>
                <td>3:40 - 4:10pm</td>
                <td style="background-color:#cae1ff">Keynote: Felix Heide</td>
                <td></td>
              </tr>
              <tr>
                <td>4:10 - 4:40pm</td>
                <td style="background-color:#cae1ff">Keynote: Christian Richardt</td>
                <td></td>
              </tr>
              <tr>
                <td>4:40 - 5:10pm</td>
                <td style="background-color:#cae1ff">Keynote: Katherine Skinner</td>
                <td></td>
              </tr>                
              <tr>
                <td>5:10 - 5:35pm</td>
                <td style="background-color:#e4ffc2">Panel Discussion<br>Moderator: TBD<br>Panelists: TBD</td>
                <td></td>
              </tr>
            </table>
          </div>
        </div>
      </section>


        <!-- ############################## -->
        <!-- invited speakers -->
        <section class="section" id="invited-speakers">
          <div class="container is-max-desktop content">
            <h2 class="title" id="speakers">Keynote Speakers</h2>
        
  <!-- 1. Keynote: Jingyi Yu -->
<a href="http://www.yu-jingyi.com/cv/" target="_blank">
  <div class="card">
    <div class="card-content">
      <div class="columns is-vcentered">
        <div class="column is-one-quarter">
          <figure class="image is-128x128">
            <img class="is-rounded" src="static/img/jingyi.png" alt="Jingyi Yu">
          </figure>
        </div>
        <div class="column">
          <p class="title is-4">Jingyi Yu</p>
          <p class="subtitle is-6">ShanghaiTech University</p>
        </div>
      </div>
      <div class="content">
        Jingyi Yu is a professor and executive dean of the School of Information Science and Technology at ShanghaiTech University. He received his B.S. from Caltech in 2000 and his Ph.D. from MIT in 2005, and he is also affiliated with the University of Delaware. His research focuses on computer vision and computer graphics, particularly in computational photography and non-conventional optics and camera designs. His research has been generously supported by the National Science Foundation (NSF), the National Institute of Health, the Army Research Office, and the Air Force Office of Scientific Research (AFOSR). He is a recipient of the NSF CAREER Award, the AFOSR YIP Award, and the Outstanding Junior Faculty Award at the University of Delaware.
      </div>
    </div>
  </div>
</a>

<!-- 2. Keynote: Sara Fridovich-Keil -->
<a href="https://sarafridov.github.io/" target="_blank">
  <div class="card">
    <div class="card-content">
      <div class="columns is-vcentered">
        <div class="column is-one-quarter">
          <figure class="image is-128x128">
            <img class="is-rounded" src="static/img/sara.png" alt="Sara Fridovich-Keil">
          </figure>
        </div>
        <div class="column">
          <p class="title is-4">Sara Fridovich-Keil</p>
          <p class="subtitle is-6">Georgia Tech</p>
        </div>
      </div>
      <div class="content">
        Sara Fridovich-Keil is an assistant professor at Georgia Tech’s Department of Electrical and Computer Engineering. She completed her postdoctoral research at Stanford University under the guidance of Gordon Wetzstein and Mert Pilanci, after earning her Ph.D. in Electrical Engineering and Computer Sciences from UC Berkeley, where she was advised by Ben Recht. Her work focuses on machine learning, signal processing, and optimization to address inverse problems in computer vision as well as in computational, medical, and scientific imaging. Her research aims to identify optimal signal representations while balancing interpretability and computational efficiency.
      </div>
    </div>
  </div>
</a>

<!-- 3. Keynote: Aviad Levis -->
<a href="https://www.aviadlevis.info/" target="_blank">
  <div class="card">
    <div class="card-content">
      <div class="columns is-vcentered">
        <div class="column is-one-quarter">
          <figure class="image is-128x128">
            <img class="is-rounded" src="static/img/aviad.png" alt="Aviad Levis">
          </figure>
        </div>
        <div class="column">
          <p class="title is-4">Aviad Levis</p>
          <p class="subtitle is-6">University of Toronto</p>
        </div>
      </div>
      <div class="content">
        Aviad Levis is an assistant professor in the Departments of Computer Science and Astronomy and Astrophysics at the University of Toronto. He is an associated faculty member at the Dunlap Institute for Astronomy and Astrophysics. His research focuses on scientific computational imaging and AI for science. Prior to that, he was a postdoctoral scholar in the Department of Computing and Mathematics at Caltech, supported by the Zuckerman and Viterbi postdoctoral fellowships, working with Katie Bouman on imaging the galactic center black hole as part of the Event Horizon Telescope collaboration. He received his Ph.D. (2020) from the Technion and B.Sc. (2013) from Ben-Gurion University. His Ph.D. thesis into tomography of clouds has paved the way for an ERC-funded space mission (CloudCT) led by his Ph.D. advisor Yoav Schechner.
      </div>
    </div>
  </div>
</a>

<!-- 4. Keynote: Felix Heide -->
<a href="https://www.cs.princeton.edu/~fheide/" target="_blank">
  <div class="card">
    <div class="card-content">
      <div class="columns is-vcentered">
        <div class="column is-one-quarter">
          <figure class="image is-128x128">
            <img class="is-rounded" src="static/img/felix.png" alt="Felix Heide">
          </figure>
        </div>
        <div class="column">
          <p class="title is-4">Felix Heide</p>
          <p class="subtitle is-6">Princeton University</p>
        </div>
      </div>
      <div class="content">
        Felix Heide is a professor of Computer Science at Princeton University and heads the Princeton Computational Imaging Lab. He received his Ph.D. from the University of British Columbia and completed his postdoctoral fellowship at Stanford University. He is recognized as a SIGGRAPH Significant New Researcher, Sloan Research Fellow, and Packard Fellow. Previously, he founded an autonomous driving startup Algolux, later acquired by Torc and Daimler Trucks. His research focuses on imaging and computer vision techniques that help devices capture details in challenging conditions, spanning optics, machine learning, optimization, computer graphics, and computer vision.
      </div>
    </div>
  </div>
</a>

<!-- 5. Keynote: Christian Richardt -->
<a href="https://richardt.name/" target="_blank">
  <div class="card">
    <div class="card-content">
      <div class="columns is-vcentered">
        <div class="column is-one-quarter">
          <figure class="image is-128x128">
            <img class="is-rounded" src="static/img/christian.png" alt="Christian Richardt">
          </figure>
        </div>
        <div class="column">
          <p class="title is-4">Christian Richardt</p>
          <p class="subtitle is-6">Meta Reality Labs</p>
        </div>
      </div>
      <div class="content">
        Christian Richardt is a Research Scientist at the Codec Avatars Lab at Meta Reality Labs in Pittsburgh, PA. He earned his Ph.D. from the University of Cambridge, where he worked on video plus depth acquisition, filtering, processing, and evaluation. Previously, he served as a Reader and EPSRC-UKRI Innovation Fellow at the University of Bath and held postdoctoral positions at Saarland University, Max-Planck-Institut für Informatik, and Inria Sophia Antipolis. His research spans image processing, computer graphics, and computer vision, combining insights from these fields to reconstruct visual information and develop novel-view synthesis techniques.
      </div>
    </div>
  </div>
</a>

<!-- 6. Keynote: Katherine Skinner -->
<a href="https://sites.google.com/umich.edu/kskin">
  <div class="card">
    <div class="card-content">
      <div class="columns is-vcentered">
        <div class="column is-one-quarter">
          <figure class="image is-128x128">
            <img class="is-rounded" src="static/img/katherine.png" alt="Katherine Skinner">
          </figure>
        </div>
        <div class="column">
          <p class="title is-4">Katherine Skinner</p>
          <p class="subtitle is-6">University of Michigan</p>
        </div>
      </div>
      <div class="content">
        Katherine Skinner is an Assistant Professor in the Department of Robotics at the University of Michigan, with a courtesy appointment in the Department of Naval Architecture and Marine Engineering. Before joining Michigan, she was a Postdoctoral Fellow at Georgia Institute of Technology in the Daniel Guggenheim School of Aerospace Engineering and the School of Earth and Atmospheric Sciences. She earned her M.S. and Ph.D. from the Robotics Institute at the University of Michigan, where she worked in the Deep Robot Optical Perception Laboratory. Her research focuses on robotics, computer vision, and machine learning to enable autonomy in dynamic, unstructured, or remote environments. Her dissertation advanced machine learning methods for underwater robotic perception, and she has collaborated with the Ford Center for Autonomous Vehicles to enhance urban perception.
      </div>
    </div>
  </div>
</a>



        <!-- call for papers -->
        <section class="section" id="Call for papers">
          <div class="container is-max-desktop content">
            <h2 class="title" id="call4paper">Call for Papers</h2>
            This workshop aims to bring together a diverse group of researchers using neural fields, gaussian splatting (GS), or other stochastically-optimized methods across a wide range of sensor domains. We recommend looking through the <a href="#relatedwork">related works</a> to explore the breadth of work in this area. We solicit <em><b>non-archival</b></em> papers (which will <em><b>not</b></em> be published in proceedings) on topics including but not limited to:
            <ul>
              <li>Neural field/GS-based reconstruction and view synthesis using non-RGB sensor measurements (LiDAR, Thermal, Event, CT, MRI, Ultrasound, Cryo-EM, Sonar, etc)</li>
              <li>Neural fields/GS for computational imaging</li>
              <li>Neural fields/GS for sensor modelling and calibration</li>
              <li>Neural fields/GS for modelling visual cues (shadows, reflections, material, etc)</li>
              <li>Applications of the above to autonomous vehicles, AR/VR/XR, robotics, medicine, scientific discovery, and beyond</li>
            </ul>
            Of the submissions, <b>three</b> will be selected by the review committee as <b>spotlight works</b>. Spotlight papers will each receive a 10-minute presentation slot in the main schedule, and all accepted papers will be able to present a poster in the workshop. We encourage submissions from both new and experienced researchers — this is a great opportunity to present your research to a broader audience, receive feedback on your work, and connect with other researchers in the field.


            <h2 class="title" id="">Style and Author Instructions</h2>
            <p>If your paper has already been accepted or published in a peer-reviewed venue <b>in the last two years</b>, you may submit it in its original format and <em><b>must specify in the submission form where it was previously accepted</b></em>. We also encourage papers accepted to CVPR 2025 to be submitted to our workshop.</p>
            <ul>
                <a href="https://forms.gle/PWz5oav7KktzMPh76"><b>> Submission Form for Previously Published Work</b> </a>
            </ul>
            <p> For new submissions:</p>
            <ul>
                
              
              
              <li>
                <b>Paper Format:</b> For new work (not previously accepted or published in a peer-reviewed venue), please use the official templates provided by <a href="https://github.com/cvpr-org/author-kit/releases">CVPR 2025</a> (8 pages max). </em>
              </li>
              <li>
                <b>Reviews:</b> Reviews will be double-blind, each submission will receive at least 2 reviews</b></em>.
              </li>
              <li>
                <b>Plagiarism:</b>  Don't do it.
              </li>
              </ul>
            
            <p class="text-justify">
              All new submissions should be anonymized. Per CVF dual submission policies, if you plan to submit the work for publication in the future, we highly encourage submitting a max four page version to this workshop to receive feedback. Supplementary material is optional with supported formats: pdf, mp4 and .zip.
            </p>
          <p>All submissions should adhere to the <a
                  href="https://cvpr.thecvf.com/Conferences/2025/AuthorGuidelines">CVPR 2025 submission
                  guidelines</a>, wherever applicable.
          </p>
          <ul>
            <a href="https://openreview.net/group?id=thecvf.com/CVPR/2025/Workshop/NFBCC"><b>> Submission Form for New, Unpublished Work</b> </a>
        </ul>
        <br>
          <p> <b>Questions?</b> Contact us at: <a href="mailto:neural-bcc@googlegroups.com">neural-bcc@googlegroups.com</a></p>
          <br>
          
          <p><strong>Paper Review Timeline:</strong>
          <table class="table">
              <tr>
                  <th scope="col">Paper Submission and supplemental material deadline</th>
                  <td> April 11, 2025</td>
              </tr>
              <tr>
                  <th scope="col">Notification to authors</th>
                  <td> May 2, 2025 </td>
              </tr>
              <tr>
                  <th scope="col">Camera ready deadline</th>
                  <td> May 16, 2025 </td>
              </tr>
          </table>

          </div>
        </section>



        <section class="section" id="Related works">
          <div class="container is-max-desktop content">
            <h2 class="title" id="relatedwork">Related Works</h2>
            Below is a collection of example works on neural fields beyond conventional cameras:
            <ul>
              <li>
                <a href="https://arxiv.org/abs/2303.16254">
                  CryoFormer: Continuous Heterogeneous Cryo-EM Reconstruction using Transformer-based Neural Representations
                </a>
                ICLR 2024
              </li>
              
              
              <li>
                <a href="https://arxiv.org/abs/2405.04662">
                  Radar Fields: Frequency-Space Neural Scene Representations for FMCW Radar
                </a>
                SIGGRAPH 2024
              </li>
              
              <li><a href="https://arxiv.org/abs/2312.14239">	
                PlatoNeRF: 3D Reconstruction in Plato's Cave via Single-View Two-Bounce Lidar</a> CVPR 2024
              </li>
              <li><a href="https://arxiv.org/abs/2312.05247">Dynamic LiDAR Re-simulation using Compositional Neural Fields</a> CVPR 2024
              </li>
              <li><a href="https://arxiv.org/abs/2305.16321">Eclipse: Disambiguating Illumination and Materials using Unintended Shadows</a> CVPR 2024 </li>
              <li>
                <a href="https://arxiv.org/pdf/2311.17396">
                  Spectral and Polarization Vision: Spectro-polarimetric Real-world Dataset
                </a>
                CVPR 2024
              </li>

              <li>
                <a href="https://arxiv.org/abs/2306.12562">
                  Neural Spectro-polarimetric Fields
                </a>
                SIGGRAPH ASIA 2023
              </li>
              <li><a href="https://arxiv.org/abs/2208.11300">E-NeRF: Neural Radiance Fields from a Moving Event Camera</a> RA-L 2023 </li>
              <li><a href="https://arxiv.org/abs/2301.10520">Ultra-NeRF: Neural Radiance Fields for Ultrasound Imaging</a> MIDL 2023 </li>
              <li><a href="https://arxiv.org/abs/2307.09555">Transient Neural Radiance Fields for Lidar View Synthesis and 3D Reconstruction</a> NeurIPS 2023 </li>
              <li><a href="https://arxiv.org/abs/2305.01643">	
                Neural LiDAR Fields for Novel View Synthesis</a> ICCV 2023
              </li>
              <li><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Shandilya_Neural_Fields_for_Structured_Lighting_ICCV_2023_paper.pdf">Neural Fields for Structured Lighting</a> ICCV 2023 </li>
              <li><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Qi_E2NeRF_Event_Enhanced_Neural_Radiance_Fields_from_Blurry_Images_ICCV_2023_paper.pdf">	
                E<sup>2</sup>NeRF: Event Enhanced Neural Radiance Fields from Blurry Images</a> ICCV 2023
              </li>
              <li><a href="https://arxiv.org/abs/2212.04531">ORCa: Glossy Objects as Radiance Field Cameras</a> CVPR 2023 </li>
              <li><a href="https://arxiv.org/abs/2305.01652">Humans as Light Bulbs: 3D Human Reconstruction from Thermal Reflection</a> CVPR 2023 </li>
              <li><a href="https://arxiv.org/abs/2304.07743">SeaThru-NeRF: Neural Radiance Fields in Scattering Media</a> CVPR 2023 </li>
              <li><a href="https://arxiv.org/abs/2206.11896">EventNeRF: Neural Radiance Fields from a Single Colour Event Camera</a> CVPR 2023 </li>
              <li><a href="https://ojs.aaai.org/index.php/AAAI/article/view/20171">Neural Interferometry: Image Reconstruction from Astronomical Interferometers Using Transformer-Conditioned Neural Fields</a> AAAI 2022 </li>
              <li><a href="https://arxiv.org/abs/2204.00628">Learning Neural Acoustic Fields</a> NeurIPS 2022 </li>
              <li><a href="https://arxiv.org/abs/2203.13458">	
                PANDORA: Polarization-Aided Neural Decomposition Of Radiance</a> ECCV 2022
              </li>                
              <li><a href="https://arxiv.org/abs/2202.01020">Medical Neural Radiance Fields for Reconstructing 3D-aware CT-Projections from a Single X-ray</a> EMBC 2022
              </li>
              <br>
            </ul>
            and many more ...


            </ul>
          </div>
        </section>
          
          


        <section class="section" id="Organizers">
          <div class="container is-max-desktop content">
            <h2 class="title" id="organizers">Organizers</h2>

        <div class="columns is-multiline">
          <!-- Ilya Chugunov -->
          <div class="column is-one-quarter">
            <a href="https://ilyac.info/">
              <div class="card">
                <div class="card-image">
                  <figure class="image">
                    <img class="is-rounded" src="./static/img/ilya.png" alt="Image of Ilya Chugunov">
                  </figure>
                </div>
                <div class="card-content">
                  <div class="media">
                    <div class="media-content" style="overflow-x: unset;">
                      <p class="title is-6" style="margin-bottom: 0vh;">Ilya Chugunov</p>
                      <p class="subtitle is-6 has-text-left has-text-grey" style="height: 0.5rem;"><br> <em>(primary)</em></p>
                      <p class="subtitle is-6" style="height: 2.2rem;"><br> Princeton</p>
                    </div>
                  </div>
                </div>
              </div>
            </a>
          </div>
        
          <!-- Tzofi Klinghoffer -->
          <div class="column is-one-quarter">
            <a href="https://tzofi.github.io/">
              <div class="card">
                <div class="card-image">
                  <figure class="image">
                    <img class="is-rounded" src="./static/img/tzofi.png" alt="Image of Tzofi Klinghoffer">
                  </figure>
                </div>
                <div class="card-content">
                  <div class="media">
                    <div class="media-content" style="overflow-x: unset;">
                      <p class="title is-6" style="margin-bottom: 0vh;">Tzofi Klinghoffer</p>
                      <p class="subtitle is-6 has-text-left has-text-grey" style="height: 0.5rem;"><br> <em>(secondary)</em></p>
                      <p class="subtitle is-6" style="height: 2.2rem;"><br>MIT Media Lab</p>
                    </div>
                  </div>
                </div>
              </div>
            </a>
          </div>

<!-- Shengyu Huang -->
<div class="column is-one-quarter">
  <a href="https://shengyuh.github.io/">
    <div class="card">
      <div class="card-image">
        <figure class="image">
          <img class="is-rounded" src="./static/img/shengyu.png" alt="Image of Shengyu Huang">
        </figure>
      </div>
      <div class="card-content">
        <div class="media">
          <div class="media-content" style="overflow-x: unset;">
            <p class="title is-6" style="margin-bottom: 2.5vh;">Shengyu Huang</p>
            <p class="subtitle is-6" style="height: 2rem;">ETH Zürich</p>
          </div>
        </div>
      </div>
    </div>
  </a>
</div>

<!-- Daniel Gilo -->
<div class="column is-one-quarter">
  <a href="https://scholar.google.com/citations?user=ARRwFY8AAAAJ&hl=en">
    <div class="card">
      <div class="card-image">
        <figure class="image">
          <img class="is-rounded" src="./static/img/daniel.png" alt="Image of Daniel Gilo">
        </figure>
      </div>
      <div class="card-content">
        <div class="media">
          <div class="media-content" style="overflow-x: unset;">
            <p class="title is-6" style="margin-bottom: 2.5vh;">Daniel Gilo</p>
            <p class="subtitle is-6" style="height: 2rem;">Technion</p>
          </div>
        </div>
      </div>
    </div>
  </a>
</div>

<!-- Wenzheng Chen -->
<div class="column is-one-quarter">
  <a href="https://wenzhengchen.github.io/">
    <div class="card">
      <div class="card-image">
        <figure class="image">
          <img class="is-rounded" src="./static/img/wenzheng.png" alt="Image of Wenzheng Chen">
        </figure>
      </div>
      <div class="card-content">
        <div class="media">
          <div class="media-content" style="overflow-x: unset;">
            <p class="title is-6" style="margin-bottom: 2.5vh;">Wenzheng Chen</p>
            <p class="subtitle is-6" style="height: 2rem;">Peking University</p>
          </div>
        </div>
      </div>
    </div>
  </a>
</div>

<!-- Akshat Dave -->
<div class="column is-one-quarter">
  <a href="https://akshatdave.github.io/">
    <div class="card">
      <div class="card-image">
        <figure class="image">
          <img class="is-rounded" src="./static/img/akshat.png" alt="Image of Akshat Dave">
        </figure>
      </div>
      <div class="card-content">
        <div class="media">
          <div class="media-content" style="overflow-x: unset;">
            <p class="title is-6" style="margin-bottom: 2.5vh;">Akshat Dave</p>
            <p class="subtitle is-6" style="height: 2rem;">MIT Media Lab</p>
          </div>
        </div>
      </div>
    </div>
  </a>
</div>

<!-- Lingjie Liu -->
<div class="column is-one-quarter">
  <a href="https://lingjie0206.github.io/">
    <div class="card">
      <div class="card-image">
        <figure class="image">
          <img class="is-rounded" src="./static/img/lingjie.png" alt="Image of Lingjie Liu">
        </figure>
      </div>
      <div class="card-content">
        <div class="media">
          <div class="media-content" style="overflow-x: unset;">
            <p class="title is-6" style="margin-bottom: 2.5vh;">Lingjie Liu</p>
            <p class="subtitle is-6" style="height: 2rem;">University of Pennsylvania</p>
          </div>
        </div>
      </div>
    </div>
  </a>
</div>

<!-- David Lindell -->
<div class="column is-one-quarter">
  <a href="https://davidlindell.com/">
    <div class="card">
      <div class="card-image">
        <figure class="image">
          <img class="is-rounded" src="./static/img/david.png" alt="Image of David Lindell">
        </figure>
      </div>
      <div class="card-content">
        <div class="media">
          <div class="media-content" style="overflow-x: unset;">
            <p class="title is-6" style="margin-bottom: 2.5vh;">David Lindell</p>
            <p class="subtitle is-6" style="height: 2rem;">University of Toronto</p>
          </div>
        </div>
      </div>
    </div>
  </a>
</div>

<!-- Or Litany -->
<div class="column is-one-quarter">
  <a href="https://orlitany.github.io/">
    <div class="card">
      <div class="card-image">
        <figure class="image">
          <img class="is-rounded" src="./static/img/or.png" alt="Image of Or Litany">
        </figure>
      </div>
      <div class="card-content">
        <div class="media">
          <div class="media-content" style="overflow-x: unset;">
            <p class="title is-6" style="margin-bottom: 2.5vh;">Or Litany</p>
            <p class="subtitle is-6" style="height: 2rem;">Technion, NVIDIA</p>
          </div>
        </div>
      </div>
    </div>
  </a>
</div>

<!-- Ramesh Raskar -->
<div class="column is-one-quarter">
  <a href="https://www.media.mit.edu/people/raskar/overview/">
    <div class="card">
      <div class="card-image">
        <figure class="image">
          <img class="is-rounded" src="./static/img/ramesh.png" alt="Image of Ramesh Raskar">
        </figure>
      </div>
      <div class="card-content">
        <div class="media">
          <div class="media-content" style="overflow-x: unset;">
            <p class="title is-6" style="margin-bottom: 2.5vh;">Ramesh Raskar</p>
            <p class="subtitle is-6" style="height: 2rem;">MIT Media Lab</p>
          </div>
        </div>
      </div>
    </div>
  </a>
</div>


        

        

        


        
        

        <section class="section" id="Previous Editions">
          <div class="container is-max-desktop content">
            <h2 class="title" id="previous">Previous Workshop Editions</h2>
            <li>
              <a href="2024/2024.html">2024 - 1st Edition @ European Conference on Computer Vision</a><br>
            </li>
          </div>
        </section>

          <footer class="footer">
            <div class="container">
              <div class="content">
                <p style="font-size: small;">
                  This website is licensed under a <a rel="license"
                    href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                    Commons Attribution-ShareAlike 4.0 International License</a>.
                  <br />
                  It borrows the source code of <a href="https://github.com/nerfies/nerfies.github.io">this website</a>, thanks to Utkarsh Sinha and Keunhong Park.
                </p>
              </div>
            </div>
          </footer>
</body>
<script src="js/jquery-2.1.1.js"></script>
<script src="js/jquery.mobile.custom.min.js"></script> <!-- Resource jQuery -->
<script src="js/main.js"></script> <!-- Resource jQuery -->

</html>
